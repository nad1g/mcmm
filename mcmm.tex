\documentclass[11pt,onecolumn]{article}
\usepackage{amsmath,amssymb,graphicx,epsfig,cite,algorithm,algorithmic,epstopdf}
\usepackage[utf8]{inputenc}
\usepackage{xfrac, nicefrac}
\usepackage{multirow}
\begin{document}
\title{Monte Carlo Methods for Mortals}
\author{Santhosh Nadig}
\maketitle
\pagebreak
\section{Introduction}
Here's a scenario: assume that you are playing solitaire (the card game) and wondering what's the probability of a successful outcome? Well, this was a question that occurred to Stan Ulam (the Polish mathematician) who was recovering from illness and playing solitaire to while away his time. He tried to solve it using combinatorics and failed. Finally, he thought of simulating a very large number of games (with cards shuffled at random, of course) and just counting the number of successes. He, and his friend, Von Neumann, who knew a thing or two about computers and programming, set out do this calculation and eventually used this method for other scientific purposes.

This is the central idea of MC. Use randomness to "simulate" a large number of experiments. Infer / calculate a quantity of interest based on the outcome of the experiments. More the number of experiments, the better!

What are Monte Carlo (MC) methods? An (over)simplified view is that they are tools for:
\begin{enumerate}
 \item Estimating (multi-dimensional) integrals
 \item Estimating probabilities
 \item ... etc.
\end{enumerate}

First, a note about estimating integrals. The usual deterministic methods of numerical integration (such as trapizoidal method) suffer from what's known as the "curse of dimensionality". That is, for a fixed error the number of function evaluations (of the integrand) grows exponentially with the "dimension of integration". In general, calculations that grow exponentially are frowned upon by engineers and scientists. At the risk of giving away the story-line, it sufficies to say that MC methods do not suffer from the aforementioned "curse." I will (probably) give examples of this later on.

\section{Estimating Probabilities} 
\subsection{Random Variables}
What is a random variable? As someone said, it is neither random nor variable. For example, let $X$ denote the resistance of a resistor (assume fixed temperature and other conditions, just for argument's sake). $X$ is an {\em unknown constant} (or, only known to the supreme fascist, SF). We do have, however, a series of measurements $x_1, x_2, \dots $ from an ohm-meter. These measurements are called realizations/observations/samples of the random variable. So, now we have a random variable $X$ and its realizations (or samples) $x_1, x_2, \dots $ and so on.

The expected value (mean, for ordinary mortals) of a random variable, $X$, is defined as
\[
E[X] = \int x p(x) dx,
\]
where $p(x)$ is the probability density function (pdf). Assume that $p(x)$ is a ``normally distributed'' (bell curve) with mean $m$ and standard deviation $\sigma$. What it means is that if you plot a histogram of  $x_1, x_2, \dots $, it resembles the bell shape centered around  $m$ and its width $\propto \sigma$.

So, in theory, we can "determine" the expected value simply by performing the integration. However, given $N$ realizations $x_1,\dots,x_N$ of $X$, we can "estimate" 
the expected value (and hence, the integral) as
\[
E[X] = m =  \int x p(x)~ dx \approx \frac{1}{N} \sum_{i=1}^N x_i.
\] 
The estimate gets better (closer to $m$) as the number of samples, N, increases.

The above idea might not be a revelation. But, let us assume that we have an integral of the kind:
\[
\int  g(x) \cdot f(x)~ dx ,
\]

where $f(x)$ is a pdf (i.e., $f(x) \ge 0$ for all $x$ and $\int f(x)~dx = 1$.
For instance, if $g(x) = x$, we obtain the expected value; And, if $g(x) = x^2$, we obtain the 
"variance", etc. These quantities are very useful - for example, the expected 
 value represents the best estimate of the random variable X and the variance 
 gives how confident we are of the estimate.

{\bf I1: MC method says:}
\[
\int g(x) \cdot f(x) ~ dx \approx \frac{1}{N} \sum_i g(x'_i)
\]
where $x'_1, x'_2,\dots$ are "sampled" from the pdf $f(x)$.
% 
% **** Example ****
% What do we mean by sampling from a pdf? Let's consider a simple example. Suppose 
% we have a "bent coin" which has a probability of coming up heads = 0.7 and 
% coming up tails = 0.3.
\end{document}
